{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf67964-acc8-40a6-9334-f2bb93915ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To bypass YouTube security, you need to manually extract your cookies.txt to paste into the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0704eb-68a7-4791-8889-a28cf7602ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install yt-dlp imageio-ffmpeg openai-whisper torch\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d399ef5-e4cc-4ae0-9d26-c0be0845ac9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import imageio_ffmpeg\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from IPython.display import display, FileLink\n",
    "import whisper\n",
    "import whisper.audio\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Set up paths\n",
    "ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "\n",
    "# Constants\n",
    "youtube_url = \"https://www.youtube.com/watch?v=oG-YaIlIj-4\"\n",
    "workspace_dir = \"/Workspace/Users/User\"\n",
    "local_tmp_path = \"/tmp/text.txt\"\n",
    "cookies_file = os.path.join(workspace_dir, \"cookies\", \"cookies.txt\")\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497ba820-f7a0-4adc-a0f0-240178ae4d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Check if ffmpeg exists: True\n",
      "Check if ffmpeg is executable: True\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=j6QxHdQ67YU\n",
      "[youtube] j6QxHdQ67YU: Downloading webpage\n",
      "[youtube] j6QxHdQ67YU: Downloading tv client config\n",
      "[youtube] j6QxHdQ67YU: Downloading tv player API JSON\n",
      "[info] j6QxHdQ67YU: Downloading 1 format(s): 251\n",
      "[download] Destination: /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.webm\n",
      "\r[download]   0.0% of    6.72MiB at   65.70KiB/s ETA 01:44\r[download]   0.0% of    6.72MiB at  155.09KiB/s ETA 00:44\r[download]   0.1% of    6.72MiB at  302.94KiB/s ETA 00:22\r[download]   0.2% of    6.72MiB at  609.42KiB/s ETA 00:11\r[download]   0.5% of    6.72MiB at    1.10MiB/s ETA 00:06\r[download]   0.9% of    6.72MiB at    1.96MiB/s ETA 00:03\r[download]   1.8% of    6.72MiB at    2.28MiB/s ETA 00:02\r[download]   3.7% of    6.72MiB at    3.61MiB/s ETA 00:01\r[download]   7.4% of    6.72MiB at    5.89MiB/s ETA 00:01\r[download]  14.9% of    6.72MiB at    9.96MiB/s ETA 00:00\r[download]  29.7% of    6.72MiB at   17.56MiB/s ETA 00:00\r[download]  59.5% of    6.72MiB at   29.18MiB/s ETA 00:00\r[download] 100.0% of    6.72MiB at   34.91MiB/s ETA 00:00\r[download] 100% of    6.72MiB in 00:00:00 at 7.82MiB/s   \n",
      "[ExtractAudio] Destination: /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.m4a\n",
      "Deleting original file /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.webm (pass -k to keep)\n",
      "\n",
      "✅ Downloaded audio: /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 8 (Debian 8.3.0-6)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzimg\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.m4a':\n",
      "  Metadata:\n",
      "    major_brand     : M4A \n",
      "    minor_version   : 512\n",
      "    compatible_brands: M4A isomiso2\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: 00:08:05.90, start: 0.000000, bitrate: 174 kb/s\n",
      "  Stream #0:0[0x1](eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 173 kb/s (default)\n",
      "      Metadata:\n",
      "        handler_name    : SoundHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.wav':\n",
      "  Metadata:\n",
      "    major_brand     : M4A \n",
      "    minor_version   : 512\n",
      "    compatible_brands: M4A isomiso2\n",
      "    ISFT            : Lavf61.1.100\n",
      "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s (default)\n",
      "      Metadata:\n",
      "        handler_name    : SoundHandler\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 pcm_s16le\n",
      "size=   22272KiB time=00:01:59.94 bitrate=1521.2kbits/s speed= 240x    \rsize=   50176KiB time=00:04:28.25 bitrate=1532.3kbits/s speed= 268x    \rsize=   85760KiB time=00:07:38.50 bitrate=1532.3kbits/s speed= 306x    \r[out#0/wav @ 0x1d4aaa00] video:0KiB audio:91105KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000084%\n",
      "size=   91105KiB time=00:08:05.89 bitrate=1536.0kbits/s speed= 313x    \n",
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-28fde234-cead-4196-8510-763271f5277b/lib/python3.11/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Converted to WAV: /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.wav\n",
      "\n",
      "✅ Transcription:\n",
      " Imagine you have puzzled with 1000 of pieces and you need to put them together to create a beautiful picture. That's what data can be like. Lots of little pieces that need to be assembled to make sense. Data breaks is like your puzzle solving buddy that helps you to put all those pieces together. Hello and welcome back to our YouTube channel. If you are joining us for the first time, don't forget to hit the subscribe button and the bell icon so you won't miss out any of our exciting content. And also I will suggest you to take up the Apache Spark training course if you are interested in this topic. The link is present in the description below. Now let's start with the topic of our video. What is data breaks? But wait, before that first we need to move on to data breaks. Data breaks were founded by the creators of Apache Spark which is an open source distributed computing system. They founded the company in 2013 and it has since become a significant player in the big data and machine learning spaces. Some of the other important contributions from the founders are MLflow and Delta Lake. MLflow is an open source platform that manages the machine learning lifecycle including experimentation, reproducibility and deployment. It provides various components to streamline the end-to-end development process. And on the other hand, Delta Lake is an open source storage layer that brings reliability to data lakes. It works over your existing data and is fully compatible with API's ViExPark, Hive and providing AC transactions using streaming and batch data processing. It provides the platform for data engineering, machine learning and analytics. Now we have to see who all can use data breaks and why. First of all we will start with data scientist. They use it for developing and training machine learning models, running data analysis and visualizing results. And it provides a collaborative workspace integration with popular machine learning libraries and notebook based programming environments. Second, we have data engineers. They use it for transforming and cleaning data, creating data pipelines and optimizing data for analysis. It supports Apache Spark for big data processing and offers integration with various data resources and things. Third and the last one we have data analysis. They use it for exploring data, creating visualizations and dashboards. It also use for running adhoccuries. It supports SQL interactive notebooks and wide array of visualization options. As we have already seen who all can use data breaks, let's start with the architecture part. The data lake house architecture is a unified platform that combines the best features of data lakes and data warehouses. It is built on three main layers. As you can see from the picture, the first layer is the bronze layer, the structure, semi-structured and unstructured data. The second one is silver layer. As you can see from the picture again, it is showing metadata and governance layer. That is the silver layer. And the third one is the golden layer. As you can see, they are mentioned, BI reports data size machine learning. That is the golden layer. So first we will start with the bronze layer. This is the raw data layer, where data from various sources in the state in its native format. It acts as a staging area for unprocessed data. Second we have silver layer. Here the raw data is clean, processed and transformed into a more consumable format. It serves as a bridge between the raw and refined data, providing a clean version of for analysis. The last one is the gold layer. The final layer where data is further aggregated and optimized for reporting and analytics. It offers a ready to use high quality data set for business users. Underpinning the architecture, the data lake, which ensures reliability, performance and security, it provides AC transactions and scalable metadata handling. Finally, it will be stored in the few of the famous cloud services like AWS and Azure. Now we will move on to the implementation part. For that first we have to discuss that data breaks has two division of platforms. First one is data engineering and the second one is machine learning. First we will start with the data engineering part. It integrates with Apache Spark, a leading open source computing system, allowing data engineers and scientists to perform complex data processing tasks. The platform's data engineering capabilities enable users to clean, transform and analyze vast amount of data efficiently. Users can work with various data sources including relational databases, no SQL stores and cloud storage, seamlessly integrating them into their analytics workflows. And on the machine learning side, data breaks offers a collaborative in-moment where data scientists can build, train and deploy machine learning models. By providing access to MLflow, a popular open source machine learning lifecycle tool, it enables tracking of experiments, model versioning and streamline deployment. The platform also supports a wide range of machine learning frameworks and libraries, such as TensorFlow, PyTorque that offers flexibility in model development. In both the platforms, we can create tables, notebooks, clusters, experiments and models. So we will start with the first one, which is creating the cluster part. So what are clusters? Clusters in data breaks are groups of computers that work together to perform tasks. By creating clusters, we can distribute our computations across multiple machines. Dating a cluster in data breaks is quite simple. Let me show you how. First we will need to log into our data breaks account, click on the cluster staff on the left hand side. Now as you can see from the screen, you can create a cluster by clicking on the create cluster button. The first we will name as I will be showing student database in this video. So I will be naming it as student database and it is also asking for the runtime version. So basically the runtime version depends on the compatibility of a system and larger the version larger will be the data sets. As you can see, so the cluster has already been created. You can also set the number of users which will be working in the clusters. Now let's start the collaborative part of the data breaks, which is the notebook. Let's create one. As you can see in the screen, the environment is already set up. We just need to put our code here. So for that, let me first paste the code here. Now let's run the code. As usual, it's taking time as they're making a database for my input code and they will arrange the student database as I have mentioned in the code. There are many options like new notebook or clone or export to DVCRKive, which you can see from the screen. We can also share files by clicking on the export option. As programming languages are concerned, we have also options in programming languages like R, Scala, Squill. As per our need, we can use the programming languages to run our code. Now we will create tables, but as you can see, we have to choose data resources from the DVFS path or S3 or other resources in the options present in the screen. So what are the other resources? They are the resources like Kinesis, Cassandra, etc. and S3 as well as DVFS, which can be used to create the directory. Sometimes it happens that we have created a lot of clusters or notebook, so the directory has been filled. So for that purpose, we can also delete few of them. As you can see from the screen, we just have to click on the clustered file and we can delete or clone it easily. So these are the few things you can start by using Databricks and there are few more remarkable ones which we will be discussing in the next video. If you are interested in knowing those remarkable features, don't forget to comment your views. Until then, happy learning. I hope you have enjoyed listening to this video. Please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest. Do look out for more videos in our playlist and subscribe to AnyRica channel to learn more. Happy learning!\n",
      "\n",
      "✅ Saved transcription to /Workspace/Users/chongjinjye@gmail.com/text.txt\n",
      "🧹 Deleted: /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.m4a\n",
      "🧹 Deleted: /Workspace/Users/chongjinjye@gmail.com/What is Databricks？ ｜ Introduction to Databricks ｜ Edureka.wav\n"
     ]
    }
   ],
   "source": [
    "def download_youtube_audio_yt_dlp(url, output_dir=workspace_dir, audio_format=\"m4a\"):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),\n",
    "        'cookiefile': cookies_file,\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': audio_format,\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'ffmpeg_location': ffmpeg_path,\n",
    "        'quiet': False,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        audio_path = ydl.prepare_filename(info).rsplit(\".\", 1)[0] + f\".{audio_format}\"\n",
    "        return audio_path\n",
    "\n",
    "def convert_to_wav_ffmpeg(input_path):\n",
    "    output_path = input_path.rsplit(\".\", 1)[0] + \".wav\"\n",
    "    subprocess.run([ffmpeg_path, \"-y\", \"-i\", input_path, output_path], check=True)\n",
    "    return output_path\n",
    "\n",
    "def load_audio_with_custom_ffmpeg(file: str, sr: int = 16000):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".raw\") as f:\n",
    "        subprocess.run([\n",
    "            ffmpeg_path,\n",
    "            \"-nostdin\", \"-threads\", \"0\", \"-i\", file,\n",
    "            \"-f\", \"s16le\", \"-ac\", \"1\", \"-acodec\", \"pcm_s16le\",\n",
    "            \"-ar\", str(sr), \"-\"\n",
    "        ], stdout=f, stderr=subprocess.DEVNULL, check=True)\n",
    "        f.seek(0)\n",
    "        raw = f.read()\n",
    "\n",
    "    audio = np.frombuffer(raw, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "    return audio\n",
    "whisper.audio.load_audio = load_audio_with_custom_ffmpeg\n",
    "\n",
    "def main():\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"Check if ffmpeg exists:\", os.path.exists(ffmpeg_path))\n",
    "    print(\"Check if ffmpeg is executable:\", os.access(ffmpeg_path, os.X_OK))\n",
    "\n",
    "    try:\n",
    "        audio_file_path = download_youtube_audio_yt_dlp(youtube_url)\n",
    "        print(f\"\\n✅ Downloaded audio: {audio_file_path}\")\n",
    "\n",
    "        wav_path = convert_to_wav_ffmpeg(audio_file_path)\n",
    "        print(f\"\\n✅ Converted to WAV: {wav_path}\")\n",
    "\n",
    "        transcribed_text = model.transcribe(wav_path)[\"text\"]\n",
    "        print(\"\\n✅ Transcription:\")\n",
    "        print(transcribed_text)\n",
    "\n",
    "        with open(f\"{workspace_dir}/text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcribed_text)\n",
    "        print(f\"\\n✅ Saved transcription to {workspace_dir}/text.txt\")\n",
    "\n",
    "    finally:\n",
    "        for f in [audio_file_path, wav_path]:\n",
    "            if f and os.path.exists(f):\n",
    "                try:\n",
    "                    os.remove(f)\n",
    "                    print(f\"Deleted: {f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {f}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc27f72-52cb-4de1-8009-79bf151324f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"done\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "YouTube_to_Text",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
